{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3226,
     "status": "ok",
     "timestamp": 1741703107421,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "_sydlc6mEgn5",
    "outputId": "594be4ba-4b73-481c-875d-f4d2b5c744e6"
   },
   "outputs": [],
   "source": [
    "# pip install Pillow pandas google-generativeai python-dotenv tqdm\n",
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1741703517759,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "Ba_93_JzB7hN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Files\\Michellenous\\A.SP\\Models\\gemini_model\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL.Image as Image\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120061,
     "status": "ok",
     "timestamp": 1741703229734,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "ietxHq_kGGQD",
    "outputId": "78eb544a-61cb-4cef-8da8-862e43a1f6da"
   },
   "outputs": [],
   "source": [
    "# Connect Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1741694773800,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "ZTq3Td60B7hQ",
    "outputId": "a70c38be-69ec-4942-ae7a-19ad4f0f6b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyC7p_USyKovd2OXynBJXrkWXG1WHmXDpxg\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API = os.getenv(\"GOOGLE_API\")\n",
    "print(GEMINI_API)\n",
    "\n",
    "def beep_sound():\n",
    "    for i in range(4):\n",
    "        winsound.Beep(380, 300)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Es6NwsoBEln5"
   },
   "outputs": [],
   "source": [
    "# prompt: create the code for importing and access google drive files from this directory\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Example: Accessing an image file\n",
    "# image_path = '/content/drive/My Drive/your_image.jpg'  # Replace with your image's path\n",
    "# try:\n",
    "#   img = Image.open(image_path)\n",
    "#   print(f\"Image '{image_path}' opened successfully.\")\n",
    "#   # Process the image here\n",
    "# except FileNotFoundError:\n",
    "#   print(f\"Error: Image file not found at '{image_path}'. Check the path.\")\n",
    "# except Exception as e:\n",
    "#   print(f\"An error occurred while opening the image: {e}\")\n",
    "\n",
    "\n",
    "# Example: Accessing a text file\n",
    "# text_file_path = '/content/drive/My Drive/your_text_file.txt' # Replace with your text file path\n",
    "# try:\n",
    "#     with open(text_file_path, 'r') as file:\n",
    "#         file_content = file.read()\n",
    "#         print(f\"Content of '{text_file_path}':\\n{file_content}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: Text file not found at '{text_file_path}'. Check the path.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred while reading the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1741703847480,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "HQBG_ACnB7hQ",
    "outputId": "7e0b6021-2795-4de6-a5e3-297b4edc7567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGES ALL ['6578.jpg', '6579.jpg', '6580.jpg', '6581.jpg', '6582.jpg', '6583.jpg', '6584.jpg', '6585.jpg', '6586.jpg', '6587.jpg', '6588.jpg', '6589.jpg', '6590.jpg', '6591.jpg', '6592.jpg', '6593.jpg', '6594.jpg', '6595.jpg', '6596.jpg', '6597.jpg', '6598.jpg', '6599.jpg', '6600.jpg', '6601.jpg', '6602.jpg', '6603.jpg', '6604.jpg', '6605.jpg', '6607.jpg', '6608.jpg', '6609.jpg', '6610.jpg', '6611.jpg', '6612.jpg', '6613.jpg', '6614.jpg', '6615.jpg', '6616.jpg', '6617.jpg', '6618.jpg', '6619.jpg', '6620.jpg', '6621.jpg', '6622.jpg', '6623.jpg', '6624.jpg', '6625.jpg', '6626.jpg', '6627.jpg', '6628.jpg', '6629.jpg', '6630.jpg', '6631.jpg', '6634.jpg', '6635.jpg', '6636.jpg', '6637.jpg', '6638.jpg', '6639.jpg', '6640.jpg', '6641.jpg', '6642.jpg', '6643.jpg', '6644.jpg', '6645.jpg', '6646.jpg', '6647.jpg', '6648.jpg', '6649.jpg', '6650.jpg', '6651.jpg', '6652.jpg', '6653.jpg', '6654.jpg', '6655.jpg', '6656.jpg', '6657.jpg', '6658.jpg', '6659.jpg', '6660.jpg', '6661.jpg', '6662.jpg', '6663.jpg', '6664.jpg', '6665.jpg', '6666.jpg', '6667.jpg', '6668.jpg', '6669.jpg', '6670.jpg', '6671.jpg', '6672.jpg', '6673.jpg', '6674.jpg', '6675.jpg', '6676.jpg', '6677.jpg', '6678.jpg', '6679.jpg', '6680.jpg', '6681.jpg', '6682.jpg', '6683.jpg', '6684.jpg', '6685.jpg', '6686.jpg', '6687.jpg', '6688.jpg', '6689.jpg', '6690.jpg', '6691.jpg', '6692.jpg', '6693.jpg', '6694.jpg', '6695.jpg', '6696.jpg', '6697.jpg', '6698.jpg', '6699.jpg', '6700.jpg', '6701.jpg', '6702.jpg', '6703.jpg', '6704.jpg', '6705.jpg', '6706.jpg', '6707.jpg', '6708.jpg', '6709.jpg', '6711.jpg', '6712.jpg', '6713.jpg', '6714.jpg', '6715.jpg', '6716.jpg', '6717.jpg', '6718.jpg', '6719.jpg', '6720.jpg', '6721.jpg', '6722.jpg', '6723.jpg', '6724.jpg', '6725.jpg', '6726.jpg', '6727.jpg', '6728.jpg', '6729.jpg', '6730.jpg', '6731.jpg', '6732.jpg', '6733.jpg', '6734.jpg', '6735.jpg', '6736.jpg', '6737.jpg', '6739.jpg', '6741.jpg', '6742.jpg', '6743.jpg', '6744.jpg', '6745.jpg', '6746.jpg', '6747.jpg', '6748.jpg', '6749.jpg', '6750.jpg', '6751.jpg', '6752.jpg', '6753.jpg', '6754.jpg', '6755.jpg', '6756.jpg', '6757.jpg', '6758.jpg', '6759.jpg', '6760.jpg', '6761.jpg', '6762.jpg', '6763.jpg', '6764.jpg', '6765.jpg', '6766.jpg', '6767.jpg', '6768.jpg', '6770.jpg', '6771.jpg', '6772.jpg', '6773.jpg', '6774.jpg', '6775.jpg', '6776.jpg', '6778.jpg', '6779.jpg', '6780.jpg', '6781.jpg', '6782.jpg', '6783.jpg', '6784.jpg', '6785.jpg', '6786.jpg', '6787.jpg', '6788.jpg', '6789.jpg', '6790.jpg', '6791.jpg', '6792.jpg', '6793.jpg', '6794.jpg', '6795.jpg', '6796.jpg', '6797.jpg', '6798.jpg', '6799.jpg', '6800.jpg', '6801.jpg', '6802.jpg', '6803.jpg', '6804.jpg', '6805.jpg', '6806.jpg', '6807.jpg', '6808.jpg', '6809.jpg', '6810.jpg', '6811.jpg', '6812.jpg', '6813.jpg', '6814.jpg', '6815.jpg', '6816.jpg', '6817.jpg', '6818.jpg', '6819.jpg', '6820.jpg', '6822.jpg', '6823.jpg', '6824.jpg', '6826.jpg', '6827.jpg', '6828.jpg', '6829.jpg', '6831.jpg', '6832.jpg', '6833.jpg', '6834.jpg', '6836.jpg', '6837.jpg', '6838.jpg', '6840.jpg', '6842.jpg', '6843.jpg', '6845.jpg', '6846.jpg', '6847.jpg', '6848.jpg', '6849.jpg', '6850.jpg', '6851.jpg', '6852.jpg', '6853.jpg', '6854.jpg', '6855.jpg', '6856.jpg', '6857.jpg', '6858.jpg', '6859.jpg', '6860.jpg', '6861.jpg', '6862.jpg', '6863.jpg', '6864.jpg', '6865.jpg', '6866.jpg', '6867.jpg', '6868.jpg', '6869.jpg', '6870.jpg', '6871.jpg', '6872.jpg', '6873.jpg', '6874.jpg', '6876.jpg', '6877.jpg', '6878.jpg']\n",
      "IMAGES DF ['6578.jpg', '6579.jpg']\n"
     ]
    }
   ],
   "source": [
    "images_path = \"./test_dataset\"\n",
    "csv_path = \"./output/gemini_generated.csv\"\n",
    "json_output = \"./output/gemini_generated.json\"\n",
    "images_list = os.listdir(images_path)\n",
    "test_df = images_list[:2]\n",
    "print(\"IMAGES ALL\", images_list)\n",
    "print(\"IMAGES DF\", test_df)\n",
    "\n",
    "# test_df = pd.read_csv(csv_path)\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1741703848781,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "obgWtqJzB7hQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyC7p_USyKovd2OXynBJXrkWXG1WHmXDpxg\n"
     ]
    }
   ],
   "source": [
    "api_keys = [GEMINI_API]\n",
    "print(api_keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1741703849907,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "Kb3WJstpB7hQ"
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key=api_keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 60154,
     "status": "error",
     "timestamp": 1741703910973,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "IkOR3M-IB7hR",
    "outputId": "9989c9fc-137f-42a3-d48a-ced6502bee91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QfwJPh3WB7hR"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "I-Ex_R6bB7hR"
   },
   "outputs": [],
   "source": [
    "def test_prompt():\n",
    "    return \"describe the image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4bJnBWQ1B7hR"
   },
   "outputs": [],
   "source": [
    "# def prompt_prep(question:str, options:list):\n",
    "def prompt_prep():\n",
    "    return \"\"\"\n",
    "    You are an expert multimodal AI assistant. Your task is to answer multiple-choice questions based on an image.\n",
    "\n",
    "    Write given question's answer in JSON FORMAT, PLEASE DO NOT FORGET JSON, ALSO START WITH THE JSON AND NOT ANY THING ELSE, FIRST CHAR IN YOUR RESPONSE IS ITS OPENING BRACE\n",
    "\n",
    "    ## **Response Format:**\n",
    "    Your response **must** strictly follow this format:\n",
    "    - Each answer should be on a new line.\n",
    "    - Use the format: `X. Y`\n",
    "      - `X` is the question number.\n",
    "      - `Y` is the letter of the correct answer.\n",
    "    - Do **not** provide explanations, reasoning, or any additional text.\n",
    "\n",
    "    ## **Example Response:**\n",
    "    {\n",
    "      \"Question-1\": \"B\",\n",
    "      \"Question-2\": \"C\",\n",
    "      \"Question-3\": \"A\",\n",
    "      \"Question-4\": \"B\",\n",
    "      \"Question-5\": \"D\"\n",
    "    }\n",
    "\n",
    "    ## **Rules:**\n",
    "    - **Do not forget to put the response inside the curly braces else THIS WILL CAUSE FIRE AND MILLIONS WILL BE HARMED**\n",
    "    - **REMEMBER TO CLOSE THE DICTIONARY WITH '}'BRACE, IT GOES AFTER THE END OF DESCRIPTION-YOU ALWAYS FORGET IT, THIS WILL CAUSE A LOT OF ISSUES**\n",
    "    - **Do not** generate any text outside the expected format.\n",
    "    - **Do not** rephrase, explain, or add comments.\n",
    "    - **Only output the answer choices as specified.\n",
    "    - If uncertain, make your best guess.\n",
    "    - Response only in the format do not include '''JSON at start and ''' at end\n",
    "\n",
    "    Now, answer the following question based on the provided image:\n",
    "\n",
    "    Question-1: How would you rate this image according to Humor? (Humor: the quality of being amusing or comic, especially as expressed in literature or speech)\n",
    "    Options:\n",
    "    A. Not funny\n",
    "    B. Funny\n",
    "    C. Very funny\n",
    "\n",
    "    Question-2: Do you find this image Sarcastic? If then Rate from the following options. (Sarcastic: marked by or given to using irony in order to mock or convey contempt.)\n",
    "    Options:\n",
    "    A. Not sarcastic\n",
    "    B. Little sarcastic\n",
    "    C. Very sarcastic\n",
    "\n",
    "    Question-3: Do you find this image Offensive? If then Rate from the following options. (Offensive: causing someone to feel resentful, upset, or annoyed.)\n",
    "    Options:\n",
    "    A. Not offensive\n",
    "    B. Slight offensive\n",
    "    C. Very offensive\n",
    "    D. Hateful offensive\n",
    "\n",
    "    Question-4: Do you find this image Motivational? Rate from the following options. (Motivational: designed to promote the desire or willingness to do or achieve something.)\n",
    "    Options:\n",
    "    A. Not motivational\n",
    "    B. Motivational\n",
    "\n",
    "    Question-5: Lastly, How would rate the image overall? select from the following options.\n",
    "    A. Very negative\n",
    "    B. Negative\n",
    "    C. Neutral\n",
    "    D. Positive\n",
    "    E. Very positive\n",
    "\n",
    "    \"\"\"\n",
    "    # .format(question, options[0], options[1],options[2], options[3])\n",
    "    \n",
    "def new_prompt():\n",
    "  return f\"\"\"\n",
    "  You are multimodal AI expert. You are given a cartoon based Meme image. Loot at the image and answer the following questions \n",
    "\n",
    "  what is shown in the image?\n",
    "  What cartoon character(s) are depicted in the meme?\n",
    "  Does the meme utilize exaggerated facial expressions or actions typical of cartoons?\n",
    "  What is the main theme or message of the meme?\n",
    "  What is the tone or mood conveyed by the cartoon characterâ€™s expression?\n",
    "  What is the text content in the meme?\n",
    "  Does the meme use visual metaphors or exaggeration (like oversized heads or limbs) typical of cartoons?\n",
    "\n",
    "  Based on your response of the question now identity the best candidate for the following multiple choice based question.\n",
    "  NOTE: Please don't include these questions in your response. Just think accordingly to these questions.\n",
    "  {prompt_prep()}\n",
    "\"\"\"\n",
    "# print(\"new_prompt\".upper(), new_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 24380,
     "status": "error",
     "timestamp": 1741702173952,
     "user": {
      "displayName": "Alvi Noor Hossain",
      "userId": "00175246537633945322"
     },
     "user_tz": -360
    },
    "id": "QR6ez0ESYXNS",
    "outputId": "fc27e292-145e-43ad-a476-b0cda35398a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 280, Image ID: 6873.jpg -> done.\n",
      "Processing index: 281, Image ID: 6874.jpg -> done.\n",
      "Processing index: 282, Image ID: 6876.jpg -> done.\n",
      "Processing index: 283, Image ID: 6877.jpg -> done.\n",
      "Processing index: 284, Image ID: 6878.jpg -> done.\n",
      "JSON ARR: [OrderedDict([('filename', '6873.jpg'), ('Question-1', 'B'), ('Question-2', 'C'), ('Question-3', 'A'), ('Question-4', 'A'), ('Question-5', 'C')]), OrderedDict([('filename', '6874.jpg'), ('Question-1', 'B'), ('Question-2', 'B'), ('Question-3', 'A'), ('Question-4', 'A'), ('Question-5', 'D')]), OrderedDict([('filename', '6876.jpg'), ('Question-1', 'B'), ('Question-2', 'C'), ('Question-3', 'A'), ('Question-4', 'A'), ('Question-5', 'D')]), OrderedDict([('filename', '6877.jpg'), ('Question-1', 'B'), ('Question-2', 'C'), ('Question-3', 'A'), ('Question-4', 'A'), ('Question-5', 'C')]), OrderedDict([('filename', '6878.jpg'), ('Question-1', 'A'), ('Question-2', 'B'), ('Question-3', 'B'), ('Question-4', 'B'), ('Question-5', 'D')])]\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=api_keys[0])\n",
    "model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "json_arr = []\n",
    "\n",
    "# images_list[39:], start=39\n",
    "for idx, item in enumerate(images_list[280:], start=280): \n",
    "    try:\n",
    "        print(f\"Processing index: {idx}, Image ID:\", item, end='')\n",
    "        image = Image.open(f\"{images_path}/{item}\")\n",
    "        # display(image)\n",
    "\n",
    "        response = model.generate_content([new_prompt(), image], stream=True)\n",
    "        # response = model.generate_content([test_prompt(), image], stream=True)\n",
    "        response.resolve()\n",
    "        # print(response.text)\n",
    "        str = response.text\n",
    "\n",
    "        \n",
    "        # Remove the first line (```json)\n",
    "        lines = str.splitlines()\n",
    "        if lines and lines[0].strip() == \"```json\":\n",
    "            lines = lines[1:]\n",
    "\n",
    "        # Remove the last line (```)\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "\n",
    "        # Rejoin the lines\n",
    "        cleaned_str = \"\\n\".join(lines)\n",
    "        # print(cleaned_str)\n",
    "        \n",
    "\n",
    "        json_data = json.loads(cleaned_str)\n",
    "        # Add Id\n",
    "        json_data = OrderedDict([('filename', item)] + list(json_data.items()))\n",
    "        json_arr.append(json_data)\n",
    "\n",
    "        # print(json_data) #print the json data to the console.\n",
    "        print(' -> done.')\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"\\t**EXCEPTION OCCURED\")\n",
    "        beep_sound()\n",
    "        break\n",
    "\n",
    "print(\"JSON ARR:\", json_arr)\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(json_arr, f)\n",
    "\n",
    "beep_sound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rY1W6pHB7hS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "285\n"
     ]
    }
   ],
   "source": [
    "# Open and read the JSON file\n",
    "try:\n",
    "    with open(json_output, 'r') as file:\n",
    "        data = json.load(file)\n",
    "except:\n",
    "    print(\"Error\")\n",
    "    \n",
    "# Print the data\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted ./output/gemini_generated.json to ./output/gemini_generated.csv\n"
     ]
    }
   ],
   "source": [
    "# JSON TO CSV\n",
    "def json_to_csv(json_file, csv_file):\n",
    "    \"\"\"\n",
    "    Converts a JSON file to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the input JSON file.\n",
    "        csv_file (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(f\"Warning: {json_file} is empty.\")\n",
    "            return\n",
    "\n",
    "        if isinstance(data, list):  # Handle list of dictionaries\n",
    "            if not data:\n",
    "                print(f\"Warning: {json_file} contains an empty list.\")\n",
    "                return\n",
    "\n",
    "            keys = data[0].keys()  # Get keys from the first dictionary\n",
    "\n",
    "            with open(csv_file, 'w', newline='', encoding='utf-8') as output_file:\n",
    "                dict_writer = csv.DictWriter(output_file, keys)\n",
    "                dict_writer.writeheader()\n",
    "                dict_writer.writerows(data)\n",
    "        elif isinstance(data, dict): #handle a single dictionary.\n",
    "            keys = data.keys()\n",
    "            with open(csv_file, 'w', newline='', encoding='utf-8') as output_file:\n",
    "                writer = csv.writer(output_file)\n",
    "                writer.writerow(keys)\n",
    "                writer.writerow(data.values())\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: {json_file} does not contain a valid JSON object (list of dicts or dict).\")\n",
    "\n",
    "        print(f\"Successfully converted {json_file} to {csv_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: JSON file not found: {json_file}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def jsonl_to_csv(jsonl_file, csv_file):\n",
    "    \"\"\"\n",
    "    Converts a JSON Lines (JSONL) file to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        jsonl_file (str): Path to the input JSONL file.\n",
    "        csv_file (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(jsonl_file, 'r', encoding='utf-8') as infile, open(csv_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            reader = map(json.loads, infile)\n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            first_row = next(reader)\n",
    "            keys = first_row.keys()\n",
    "            writer.writerow(keys)\n",
    "            writer.writerow(first_row.values())\n",
    "\n",
    "            for row in reader:\n",
    "                writer.writerow(row.values())\n",
    "\n",
    "        print(f\"Successfully converted {jsonl_file} to {csv_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: JSONL file not found: {jsonl_file}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSONL format in {jsonl_file}\")\n",
    "    except StopIteration:\n",
    "        print(f\"Warning: {jsonl_file} is empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example Usage:\n",
    "#For a regular json file.\n",
    "json_file = json_output\n",
    "csv_file = csv_path\n",
    "json_to_csv(json_file, csv_file)\n",
    "# print(json_file)\n",
    "# print(csv_file)\n",
    "\n",
    "#For a Jsonl file.\n",
    "# jsonl_to_csv('input.jsonl', 'output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = test_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeEaJcY2B7hS"
   },
   "outputs": [],
   "source": [
    "# column_name = 'description'\n",
    "# test_df.loc[:,column_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au0XNtOOB7hS",
    "outputId": "30908eb3-8734-44a4-d37c-2d0d9a53f7a1"
   },
   "outputs": [],
   "source": [
    "# slicedf = test_df#.iloc[0:53]\n",
    "# slicedf.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCB08LTSB7hS"
   },
   "outputs": [],
   "source": [
    "# def generate_gemini(question: str, image: str, options:list):\n",
    "\n",
    "\n",
    "#     image = Image.open(f\"{image_path}/{image}\")\n",
    "\n",
    "#     genai.configure(api_key=api_keys[random.randint(0,8)])\n",
    "#     model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "#     response = model.generate_content([prompt_prep(question, options), image], stream=True)\n",
    "#     response.resolve()\n",
    "#     time.sleep(1)\n",
    "\n",
    "#     return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze4Ly_OjB7hS",
    "outputId": "e13936f3-44f9-465c-9c7b-7b8c362819f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: CHITTRON_7834.png and idx:  0  answer:  answer: 4\n",
      "done: CHITTRON_3642.png and idx:  1  answer:  answer: 2\n",
      "done: BORNON_215.jpg and idx:  2  answer:  answer: 1\n",
      "done: BNATURE_3915.jpg and idx:  3  answer:  answer: 1\n",
      "done: CHITTRON_496.png and idx:  4  answer:  answer: 1\n",
      "done: CHITTRON_4688.png and idx:  5  answer:  answer: 4\n",
      "done: BORNON_3396.jpg and idx:  6  answer:  answer: 1\n",
      "done: BORNON_324.jpg and idx:  7  answer:  answer: 1\n",
      "\n",
      "done: BORNON_3694.jpg and idx:  8  answer:  answer: 1\n",
      "done: CHITTRON_4894.png and idx:  9  answer:  answer: 1\n"
     ]
    }
   ],
   "source": [
    "# for idx, row in (slicedf.iterrows()):\n",
    "\n",
    "\n",
    "#     image = row['filename']\n",
    "\n",
    "#     image = Image.open(f\"{image_path}/{image}\")\n",
    "\n",
    "#     question = row[\"question\"]\n",
    "#     options = ast.literal_eval(row[\"options\"])\n",
    "\n",
    "\n",
    "#     genai.configure(api_key=api_keys[(idx)%12])\n",
    "#     model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "#     response = model.generate_content([prompt_prep(question, options), image], stream=True)\n",
    "#     # response = model.generate_content([test_prompt(), image], stream=True)\n",
    "#     response.resolve()\n",
    "\n",
    "#     test_df.loc[test_df['filename'] == str(row['filename']), column_name] = response.text\n",
    "\n",
    "#     print(f\"done: {row['filename']} and idx: \",idx,\" answer: \", str(response.text))\n",
    "\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enCpoRYwB7hS",
    "outputId": "ac381fe5-ec2a-41e7-a376-6c1e881e3f5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The photo features an eye-level shot of two people, a woman and a child, in a dimly lit room. The child is seated on the floor, dressed in a vibrant red outfit with gold accents, and has a pink headband. She wears a lanyard around her neck. The woman, kneeling beside the child, wears a bright yellow garment. Light streams in, casting shadows and highlighting the subjects against the dark background.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_df.loc[0, column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Hz9TClrB7hT",
    "outputId": "4b7d5769-5a5b-4b41-a1a9-41aabc9f24ac"
   },
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLua5ekDB7hT"
   },
   "outputs": [],
   "source": [
    "# def generate_answer(item):\n",
    "#     image = item['filename']\n",
    "#     question = item[\"question\"]\n",
    "#     options = ast.literal_eval(item[\"options\"])\n",
    "#     response = generate_gemini(question, image, options)\n",
    "#     print(1)\n",
    "\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NRm1pRnB7hT",
    "outputId": "72ecb349-5cc1-455e-e0db-8b1b5208969f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini_response\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m options \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[42], line 17\u001b[0m, in \u001b[0;36mgenerate_gemini\u001b[0;34m(question, image, options)\u001b[0m\n\u001b[1;32m     15\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mapi_keys[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m8\u001b[39m)])\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/gemini-1.5-pro-latest\u001b[39m\u001b[38;5;124m'\u001b[39m)    \n\u001b[0;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_prep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m response\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m     19\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/generativeai/generative_models.py:325\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mrewrite_stream_error():\n\u001b[0;32m--> 325\u001b[0m         iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1143\u001b[0m, in \u001b[0;36mGenerativeServiceClient.stream_generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/vqa-next-7sdNAgBm-py3.10/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:174\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StreamingResponseIterator(\n\u001b[1;32m    171\u001b[0m         result, prefetch_first_result\u001b[38;5;241m=\u001b[39mprefetch_first\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "# test_df[\"gemini_response\"] = test_df.apply(generate_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLYRLwKaB7hT",
    "outputId": "d274f500-5444-4722-b14e-fd954aae0b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename           0\n",
       "question           0\n",
       "options            0\n",
       "answer             0\n",
       "category           0\n",
       "id                 0\n",
       "openai_response    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNJbHWZMB7hT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGr2cI3HB7hT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
